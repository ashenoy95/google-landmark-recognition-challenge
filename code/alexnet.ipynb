{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/ashenoy/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import pickle\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:08<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "path = '/share/jproject/fg538/r-006-gpu-4/data'\n",
    "data = []\n",
    "\n",
    "with h5py.File('{}/train-data-1000.h5'.format(path), 'r') as f:\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        arr = f['train-images-{}'.format(i)][:]\n",
    "        for a in arr:\n",
    "            data.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "with open('{}/train-labels-1000.pkl'.format(path), 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "\n",
    "for folder in os.listdir('{}/train/'.format(path)):\n",
    "    classes.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_map = {classes[i]:i for i in range((len(classes)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_100 = [classes_map[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels_100, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros((1, 227,227,3)).astype(np.float32)\n",
    "train_y = np.zeros((1, 100))\n",
    "xdim = train_x.shape[1:]\n",
    "ydim = train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_data = np.load(\"/share/jproject/fg538/r-006-gpu-4/alexnet.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  padding=\"VALID\", group=1):\n",
    "    '''From https://github.com/ethereon/caffe-tensorflow\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    assert c_i%group==0\n",
    "    assert c_o%group==0\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    \n",
    "    \n",
    "    if group==1:\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        input_groups =  tf.split(input, group, 3)   #tf.split(3, group, input)\n",
    "        kernel_groups = tf.split(kernel, group, 3)  #tf.split(3, group, kernel) \n",
    "        output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n",
    "        conv = tf.concat(output_groups, 3)          #tf.concat(3, output_groups)\n",
    "    return  tf.reshape(tf.nn.bias_add(conv, biases), [-1]+conv.get_shape().as_list()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None,) + xdim)\n",
    "y_ = tf.placeholder(tf.float32, [None, 100])\n",
    "\n",
    "\n",
    "#conv1\n",
    "#conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4\n",
    "conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "conv1_in = conv(x, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
    "conv1 = tf.nn.relu(conv1_in)\n",
    "\n",
    "#lrn1\n",
    "#lrn(2, 2e-05, 0.75, name='norm1')\n",
    "radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                                          depth_radius=radius,\n",
    "                                          alpha=alpha,\n",
    "                                          beta=beta,\n",
    "                                          bias=bias)\n",
    "\n",
    "#maxpool1\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "\n",
    "#conv2\n",
    "#conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv2 = tf.nn.relu(conv2_in)\n",
    "\n",
    "\n",
    "#lrn2\n",
    "#lrn(2, 2e-05, 0.75, name='norm2')\n",
    "radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                                          depth_radius=radius,\n",
    "                                          alpha=alpha,\n",
    "                                          beta=beta,\n",
    "                                          bias=bias)\n",
    "\n",
    "#maxpool2\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool2')                                                  \n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "#conv3\n",
    "#conv(3, 3, 384, 1, 1, name='conv3')\n",
    "k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1\n",
    "conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv3 = tf.nn.relu(conv3_in)\n",
    "\n",
    "#conv4\n",
    "#conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2\n",
    "conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv4 = tf.nn.relu(conv4_in)\n",
    "\n",
    "\n",
    "#conv5\n",
    "#conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv5 = tf.nn.relu(conv5_in)\n",
    "\n",
    "#maxpool5\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "#fc6\n",
    "#fc(4096, name='fc6')\n",
    "fc6W = tf.Variable(net_data[\"fc6\"][0])\n",
    "fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(np.prod(maxpool5.get_shape()[1:]))]), fc6W, fc6b)\n",
    "\n",
    "#fc7\n",
    "#fc(4096, name='fc7')\n",
    "fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "fc7 = tf.nn.relu_layer(fc6, fc7W, fc7b)\n",
    "\n",
    "\n",
    "#fc8\n",
    "#fc(1000, relu=False, name='fc8')\n",
    "fc8W = tf.Variable(tf.random_normal([4096, 100], stddev=tf.sqrt(2/(4096+100)), mean=0))\n",
    "fc8b = tf.Variable(tf.zeros([100]))\n",
    "fc8 = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "\n",
    "#prob\n",
    "#softmax(name='prob'))\n",
    "prob = tf.nn.softmax(fc8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=fc8))\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 128\n",
    "indices = np.arange(len(x_train))\n",
    "model_path = \"/share/jproject/fg538/r-006-gpu-4/models/alexnet/alexnet_1000.ckpt\"\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /share/jproject/fg538/r-006-gpu-4/models/alexnet/alexnet_1000.ckpt\n",
      "\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n",
      "batch: 8\n",
      "batch: 9\n",
      "batch: 10\n",
      "batch: 11\n",
      "batch: 12\n",
      "batch: 13\n",
      "batch: 14\n",
      "batch: 15\n",
      "batch: 16\n",
      "batch: 17\n",
      "batch: 18\n",
      "batch: 19\n",
      "batch: 20\n",
      "batch: 21\n",
      "batch: 22\n",
      "0.8592322035071183 4.538391260603882\n"
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    y_train_one_hot = sess.run(tf.one_hot(y_train, 100))\n",
    "    y_test_one_hot = sess.run(tf.one_hot(y_test, 100))\n",
    "    \n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    with open(\"/share/jproject/fg538/r-006-gpu-4/models/alexnet/losses.csv\", 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            avg_cost = 0\n",
    "            temp_count = 0\n",
    "            \n",
    "            for j in range(0, len(indices), mini_batch_size): \n",
    "                if temp_count%100==0:\n",
    "                    print('batch:', temp_count+1)\n",
    "                temp_count += 1\n",
    "                \n",
    "                batch_indices = indices[j:j+64]\n",
    "                batch_xs = x_train[batch_indices]\n",
    "                batch_ys = y_train_one_hot[batch_indices]\n",
    "\n",
    "                _, cost = sess.run([train_step, cross_entropy], feed_dict = {x : batch_xs, y_ : batch_ys})\n",
    "                \n",
    "                avg_cost += cost\n",
    "                \n",
    "            avg_cost /= temp_count\n",
    "\n",
    "            print(\"Epoch:\", '%02d'%(i+1), \n",
    "                  \"\\t train loss={:.9f}\".format(avg_cost),\n",
    "                  \"\\t time:{}\\n\".format(time.time()-start_time))\n",
    "\n",
    "            saver.save(sess, model_path)\n",
    "\n",
    "            writer.writerow((i, avg_cost))\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(prob, 1), tf.argmax(y_, 1))\n",
    "        accuracy = 100*tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        top_5 = 100*tf.reduce_mean(tf.cast(tf.nn.in_top_k(predictions=prob, targets=tf.argmax(y_, 1), k=5),\n",
    "                                           tf.float32))\n",
    "        \n",
    "        top1 = 0\n",
    "        top5 = 0\n",
    "        test_count = 0\n",
    "        print()\n",
    "        for k in range(0, len(x_test), 1000):\n",
    "            print('batch:', test_count+1)\n",
    "            test_count += 1\n",
    "            top1 += len(x_test[k:k+1000])*sess.run(accuracy, feed_dict={x: x_test[k:k+1000], \n",
    "                                                                        y_: y_test_one_hot[k:k+1000]})\n",
    "            top5 += len(x_test[k:k+1000])*sess.run(top_5, feed_dict={x: x_test[k:k+1000], \n",
    "                                                                     y_: y_test_one_hot[k:k+1000]})\n",
    "        \n",
    "        print(top1/len(x_test), top5/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
